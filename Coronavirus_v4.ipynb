{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install tensorflow=2.0 python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0-preview is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "class Person:\n",
    "    def __init__(self):\n",
    "        self.work_loc = 0\n",
    "        self.home_loc = 0\n",
    "        self.store_loc = 0\n",
    "        self.has_virus_num_steps = 0\n",
    "        self.age = 0\n",
    "        self.is_dead = 0\n",
    "        self.is_contagious = 0\n",
    "        self.degree_of_synonyms = 0\n",
    "        self.day_of_week_go_shopping = 0\n",
    "        self.does_follow_lockdown = 0\n",
    "        self.went_to_store = 0\n",
    "        self.will_go_to_hospital = 0\n",
    "        \n",
    "class Environment:\n",
    "       \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self) -> List[float]:\n",
    "        self.current_steps = 0\n",
    "        self.current_day = 0\n",
    "        self.people = []\n",
    "        self.people_current_location = {}\n",
    "        self.hospital_loc = 400    \n",
    "        \n",
    "        self.num_people_infected = []\n",
    "        self.num_people_died = []\n",
    "        self.num_people_recovered = []\n",
    "        \n",
    "        self.total_num_people_infected = []\n",
    "        self.total_num_people_died = []\n",
    "        self.total_num_people_recovered = []\n",
    "        \n",
    "        #for i in range(111):\n",
    "        for i in range(441):\n",
    "            self.people_current_location[i] = []\n",
    "                \n",
    "        \n",
    "        #for i in range(100):\n",
    "        for i in range(1200):\n",
    "            person = Person()\n",
    "            person.home_loc = random.randint(0,399)\n",
    "            person.work_loc = random.randint(400,438)\n",
    "            person.store_loc = random.randint(439,440)\n",
    "            person.age = random.randint(0,100)\n",
    "            person.day_of_week_go_shopping = random.randint(1, 7)\n",
    "            person.does_follow_lockdown = random.randint(0,9)\n",
    "            person.will_go_to_hospital = random.randint(0,9)\n",
    "            \n",
    "            self.people.append(person)\n",
    "                    \n",
    "            starting_loc = random.randint(0,10) \n",
    "            \n",
    "            if starting_loc < 8:\n",
    "                self.people_current_location[person.home_loc].append(person)\n",
    "            elif starting_loc < 9:\n",
    "                self.people_current_location[person.work_loc].append(person)\n",
    "            else:\n",
    "                person.went_to_store = 1\n",
    "                self.people_current_location[person.store_loc].append(person)\n",
    "                \n",
    "        \n",
    "        for i in range(2):\n",
    "            self.people[i].has_virus_num_steps = 1\n",
    "            self.people[i].is_contagious = 1\n",
    "            self.people[i].degree_of_synonyms = random.randint(1,10)\n",
    "        \n",
    "        self.num_people_infected.append(2)\n",
    "        self.total_num_people_infected.append(2)\n",
    "        self.num_people_died.append(0)\n",
    "        self.total_num_people_died.append(0)\n",
    "        self.num_people_recovered.append(0)\n",
    "        self.total_num_people_recovered.append(0)\n",
    "        \n",
    "        return self.get_observation()\n",
    "                \n",
    "    def get_observation(self):\n",
    "        observations = []\n",
    "        \n",
    "        observations.append(self.current_steps)\n",
    "        \n",
    "        for person in sorted(self.people, key = lambda x: (x.is_contagious, x.is_dead)):\n",
    "            #observations.append(person.work_loc)\n",
    "            #observations.append(person.home_loc)\n",
    "            #observations.append(person.store_loc)\n",
    "            #observations.append(person.has_virus_num_steps)\n",
    "            #observations.append(person.age)\n",
    "            observations.append(person.is_dead)\n",
    "            observations.append(person.is_contagious)\n",
    "            #observations.append(person.degree_of_synonyms)\n",
    "            #observations.append(person.day_of_week_go_shopping)\n",
    "            #observations.append(person.does_follow_lockdown)\n",
    "            #observations.append(person.went_to_store)\n",
    "            #observations.append(person.will_go_to_hospital)\n",
    "                \n",
    "        #return np.array(observations).reshape(50,50,1).astype('float32')\n",
    "        return np.array(observations)\n",
    "        \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return [0, 1, 2, 3]\n",
    "\n",
    "    def is_done(self) -> bool:\n",
    "        return self.current_steps > 10000\n",
    "\n",
    "    def step(self, action: int, iteration: int, episode: int) -> float:\n",
    "        \n",
    "        \n",
    "        self.current_steps += 1\n",
    "        \n",
    "        self.current_day = int(self.current_steps / 3) + 1\n",
    "        \n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "                \n",
    "        self.move_people(action)\n",
    "                \n",
    "        reward = 0                \n",
    "        \"\"\"\n",
    "        for person in self.people:\n",
    "            if person.is_dead == 1:\n",
    "                reward -= .5\n",
    "            elif person.is_contagious == 1:\n",
    "                reward -= .5\n",
    "            elif person.is_contagious == 0 and person.has_virus_num_steps > 0:\n",
    "                reward -= .5    \n",
    "            else:\n",
    "                reward += 1\n",
    "        \"\"\"\n",
    "        for person in self.people:\n",
    "            if person.has_virus_num_steps == 0:\n",
    "                reward += 1 \n",
    "            else:\n",
    "                reward -= 1\n",
    "        #reward = (reward*self.current_steps + 1)\n",
    "        \n",
    "        obs = self.get_observation()\n",
    "        done = self.is_done()\n",
    "        info = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if episode == 9: #(episode == 9) or (iteration == 0 and episode == 0):\n",
    "            #clear_output(wait=True)\n",
    "            #print(f)  # use display(f)\n",
    "            #print(self.people_current_location)\n",
    "            \n",
    "            print(\"iteration\",iteration,\"Steps\",self.current_steps, \"infected\",self.num_people_infected[-1], \\\n",
    "                  \"died\",self.num_people_died[-1] \\\n",
    "                  ,\"recovered\",self.num_people_recovered[-1], \\\n",
    "                  \"total infected\",self.total_num_people_infected[-1],\"total died\",self.total_num_people_died[-1] \\\n",
    "                  ,\"total recovered\",self.total_num_people_recovered[-1])\n",
    "\n",
    "            if iteration == 0 or iteration == 10:\n",
    "                X = []\n",
    "                Y = []\n",
    "                C = []\n",
    "\n",
    "                for current_loc in self.people_current_location.keys():\n",
    "                    for i in range(len(self.people_current_location[current_loc])):\n",
    "                        X.append(int((current_loc)/21)+1)\n",
    "                        Y.append(int((current_loc)%21)+1)\n",
    "                        if self.people_current_location[current_loc][i].has_virus_num_steps == 0:\n",
    "                            C.append('g')\n",
    "                        elif self.people_current_location[current_loc][i].is_contagious == 1:\n",
    "                            C.append('r')\n",
    "                        else:\n",
    "                            C.append('y')\n",
    "\n",
    "                plt.scatter(X, Y, s=75, c=C, alpha=.5)\n",
    "\n",
    "                plt.xlim(0, 22)\n",
    "                plt.xticks([])\n",
    "                plt.ylim(0, 22)\n",
    "                plt.yticks([])\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                #sleep(2)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def move_people(self, action):\n",
    "        \n",
    "        num_people_infected = 0\n",
    "        num_people_died = 0\n",
    "        num_people_recovered = 0\n",
    "        \n",
    "        people_current_location = {}\n",
    "        for i in range(441):\n",
    "            people_current_location[i] = []\n",
    "\n",
    "        for current_loc in self.people_current_location.keys():\n",
    "            for i in range(len(self.people_current_location[current_loc])):\n",
    "                person = self.people_current_location[current_loc][i]\n",
    "                                \n",
    "                if person.has_virus_num_steps > 0:\n",
    "                    person.has_virus_num_steps += 1\n",
    "                \n",
    "                if person.has_virus_num_steps > 42 and person.degree_of_synonyms > 8:\n",
    "                    num_people_died += 1\n",
    "                    person.is_dead = 1\n",
    "                    continue\n",
    "                elif person.has_virus_num_steps > 42 and person.is_contagious == 1:\n",
    "                    num_people_recovered += 1\n",
    "                    person.is_contagious = 0\n",
    "                elif person.has_virus_num_steps > 15 and person.degree_of_synonyms > 6 and \\\n",
    "                    person.will_go_to_hospital > 4:\n",
    "                    people_current_location[self.hospital_loc].append(person)\n",
    "                    continue\n",
    "                # 3\n",
    "                if action == 1: \n",
    "                    people_current_location[person.home_loc].append(person)\n",
    "                elif action == 2 and person.does_follow_lockdown < 8: \n",
    "                    people_current_location[person.home_loc].append(person)\n",
    "                elif action == 3 and person.does_follow_lockdown < 8 and \\\n",
    "                    (self.current_day % person.day_of_week_go_shopping) == 0 and \\\n",
    "                    person.went_to_store == 0:\n",
    "                    person.went_to_store = 1\n",
    "                    people_current_location[person.store_loc].append(person)\n",
    "                elif action == 3 and person.does_follow_lockdown < 8 and \\\n",
    "                    (self.current_day % person.day_of_week_go_shopping) == 0 and \\\n",
    "                    person.went_to_store == 1:\n",
    "                    people_current_location[person.home_loc].append(person)\n",
    "                elif action == 3 and person.does_follow_lockdown < 8 and \\\n",
    "                    (self.current_day % person.day_of_week_go_shopping) != 0:\n",
    "                    person.went_to_store = 0\n",
    "                    people_current_location[person.home_loc].append(person)\n",
    "                else:\n",
    "                    if self.people_current_location[current_loc][i].home_loc == current_loc:\n",
    "                        people_current_location[person.work_loc].append(person)\n",
    "                    elif self.people_current_location[current_loc][i].work_loc == current_loc and \\\n",
    "                        person.day_of_week_go_shopping == self.current_day:\n",
    "                        people_current_location[person.store_loc].append(person)\n",
    "                    elif self.people_current_location[current_loc][i].store_loc == current_loc:\n",
    "                        people_current_location[person.home_loc].append(person)\n",
    "                    else:\n",
    "                        people_current_location[person.home_loc].append(person)\n",
    "                        \n",
    "        for current_loc in people_current_location.keys():\n",
    "            if next((x for x in people_current_location[current_loc] if x.is_contagious == 1), None) != None:\n",
    "                for i in range(len(people_current_location[current_loc])):\n",
    "                    if people_current_location[current_loc][i].has_virus_num_steps == 0 and \\\n",
    "                        ((random.randint(0, 9) >= 8 and people_current_location[current_loc][i].store_loc == current_loc) or \\\n",
    "                         (random.randint(0, 9) >= 5 and people_current_location[current_loc][i].work_loc == current_loc and current_loc != 400) or \\\n",
    "                        (random.randint(0, 1000) >= 1 and people_current_location[current_loc][i].work_loc == current_loc and current_loc == 400) or \\\n",
    "                        (people_current_location[current_loc][i].home_loc == current_loc)):\n",
    "                        people_current_location[current_loc][i].has_virus_num_steps = 1\n",
    "                        people_current_location[current_loc][i].is_contagious = 1\n",
    "                        people_current_location[current_loc][i].degree_of_synonyms = random.randint(1,10)\n",
    "                        num_people_infected += 1\n",
    "                        break\n",
    "        \n",
    "        self.people_current_location = people_current_location\n",
    "        \n",
    "        self.num_people_infected.append(num_people_infected)\n",
    "        self.total_num_people_infected.append(num_people_infected+self.total_num_people_infected[-1])\n",
    "        self.num_people_died.append(num_people_died)\n",
    "        self.total_num_people_died.append(num_people_died+self.total_num_people_died[-1])\n",
    "        self.num_people_recovered.append(num_people_recovered)\n",
    "        self.total_num_people_recovered.append(num_people_recovered+self.total_num_people_recovered[-1])\n",
    "                        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "        \n",
    "        \n",
    "    def step(self, env: Environment):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        obs, reward, done, info = env.step(random.choice(actions))\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        print(reward, self.total_reward)\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "\n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn, iteration, episode):\n",
    "    global random_picks\n",
    "    global model_picks\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        #print(np.newaxis)\n",
    "        model_probas = model(obs[np.newaxis])\n",
    "\n",
    "        model_class = tf.math.argmax(model_probas, axis=1, output_type=tf.dtypes.int32)[0]\n",
    "\n",
    "\n",
    "        #print(obs)\n",
    "        #left_proba = model.predict(obs)\n",
    "        rand_probas = tf.random.uniform([1, 4])\n",
    "        #rand_probas = rand_probas/tf.keras.backend.sum(rand_probas)\n",
    "        \n",
    "        #if episode == 9:\n",
    "        #rand_probas = rand_probas * ((10-iteration)*.5)\n",
    "            \n",
    "        rand_class = tf.math.argmax(rand_probas, axis=1, output_type=tf.dtypes.int32)[0]\n",
    "                \n",
    "        if (10-(iteration)) > episode:\n",
    "            action = rand_class\n",
    "            y_target = rand_probas\n",
    "            random_picks+=1\n",
    "        else:\n",
    "            if model_probas[0, model_class] > rand_probas[0, rand_class]:\n",
    "                action = model_class\n",
    "                y_target = model_probas\n",
    "                model_picks+=1\n",
    "            else:\n",
    "                action = rand_class\n",
    "                y_target = rand_probas\n",
    "                random_picks+=1\n",
    "                \n",
    "        pred = model_probas\n",
    "        #y_target = to_categorical(action, num_classes=3)\n",
    "        #pred = to_categorical(model_class, num_classes=3)\n",
    "        #y_target = tf.keras.backend.one_hot(action, num_classes=3)\n",
    "        #pred = tf.keras.backend.one_hot(model_class, num_classes=3)\n",
    "    \n",
    "        #print(y_target, pred)\n",
    "        tape.watch(y_target)\n",
    "        tape.watch(pred)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, pred))\n",
    "        #print(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    #print(grads)\n",
    "    obs, reward, done, info = env.step(int(action), iteration, episode)\n",
    "    #obs, reward, done, info = env.step(1, iteration)\n",
    "    return obs, reward, done, grads, int(action)\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn, iteration):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    \n",
    "    action_history_map[iteration] = {}\n",
    "    action_history_map[iteration][0] = 0\n",
    "    action_history_map[iteration][1] = 0\n",
    "    action_history_map[iteration][2] = 0\n",
    "    action_history_map[iteration][3] = 0\n",
    "\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads, action = play_one_step(env, obs, model, loss_fn, iteration, episode)\n",
    "            action_history_map[iteration][action] = action_history_map[iteration][action] + 1\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            #print(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    #for step in range(len(rewards) - 2, -1, -1):\n",
    "    #    discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "\n",
    "for z in range(5):\n",
    "    action_history_map = {}\n",
    "    random_picks = 0\n",
    "    model_picks = 0\n",
    "\n",
    "    n_iterations = 11\n",
    "    n_episodes_per_update = 15\n",
    "    n_max_steps = 90\n",
    "\n",
    "    discount_rate = 1 #0.95\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    loss_fn = keras.losses.categorical_crossentropy\n",
    "\n",
    "    #np.random.seed(42)\n",
    "    #tf.random.set_seed(42)\n",
    "\n",
    "    env = Environment()\n",
    "    rewards = []\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "\n",
    "        keras.layers.Dense(128, activation=\"relu\", input_shape=[len(env.get_observation())]),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(4, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "    #tf.keras.backend.set_floatx('float16')\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=env.get_observation().shape),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(3, activation=\"softmax\" ),\n",
    "    ])\n",
    "    \"\"\"\n",
    "    #model.kernel_initializer=\"he_normal\"\n",
    "    action_history = []\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards, all_grads = play_multiple_episodes(\n",
    "            env, n_episodes_per_update, n_max_steps, model, loss_fn, iteration)\n",
    "        total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "\n",
    "        #print(\"Iteration: {}, mean rewards: {:.1f}\\n\".format(          # Not shown\n",
    "        #    iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "        #sleep(3)\n",
    "        rewards.append(total_rewards / n_episodes_per_update)\n",
    "        #all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "        #                                                   discount_rate)\n",
    "        all_final_rewards = all_rewards\n",
    "        all_mean_grads = []\n",
    "        for var_index in range(len(model.trainable_variables)):\n",
    "            mean_grads = tf.reduce_mean(\n",
    "                [final_reward * all_grads[episode_index][step][var_index]\n",
    "                 for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                     for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "            \n",
    "            all_mean_grads.append(mean_grads)\n",
    "        optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "        print(action_history_map, \"R\", random_picks, \"M\", model_picks)\n",
    "    print(action_history_map)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Iteration\", fontsize=14)\n",
    "    plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "    #save_fig(\"dqn_rewards_plot\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
